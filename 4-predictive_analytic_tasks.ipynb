{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive analytics challenge\n",
    "I've chosen time series (forecast) and anomaly detection for my 2 projects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqldf\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import sklearn as sk\n",
    "import helper.functions as hf\n",
    "from datetime import datetime\n",
    "import holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make logger\n",
    "start = datetime.now()\n",
    "logger = hf.make_logger('4-predictive_analytic_tasks_explore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "col_types = {\n",
    "    'notification_number':str,\n",
    "    'reference_number':str\n",
    "    }\n",
    "date_cols = ['creation_timestamp','completion_timestamp']\n",
    "df = pd.read_csv('data/raw/sr_hex.csv', parse_dates=date_cols,dtype=col_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparations\n",
    "Calendar dates are often usefull added info.\n",
    "We can get year, month and week (needed for aggregation) but we can also pull public holidays.\n",
    "So we can try and make some values here for each of these.\n",
    "\n",
    "For forecasts the 0 hexes might not be useful. They aren't a clustered area with a shared pattern.\n",
    "We'll remove these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date calculations\n",
    "df = df[df['h3_level8_index'] != '0']\n",
    "df['create_yrweek'] = df['creation_timestamp'].dt.year.astype(str) + df['creation_timestamp'].dt.isocalendar().week.astype(str).map(\"{:0>2}\".format)\n",
    "df['create_date'] = pd.to_datetime(df['creation_timestamp'].dt.date)\n",
    "df['date_diff'] = (df['completion_timestamp'] - df['creation_timestamp']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['create_yrweek'].max(), df['create_yrweek'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find dates for calendar years based on date range for df\n",
    "sa_calendar = holidays.SouthAfrica(years=[2020,2021,2022])\n",
    "sa_calendar = pd.DataFrame(sa_calendar.items(), columns=['date', 'desc'])\n",
    "sa_calendar['date'] = pd.to_datetime(sa_calendar['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's join this info to the data\n",
    "df['create_pubday'] = df.merge(sa_calendar, how='left', left_on='create_date', right_on='date')['desc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take the cols we need for forecast only\n",
    "df_f = df[['h3_level8_index','create_yrweek','create_date','create_pubday']]\n",
    "\n",
    "query = '''\n",
    "select h3_level8_index,\n",
    "       create_yrweek,\n",
    "       count(distinct create_pubday) pubday,\n",
    "       count(*) req\n",
    "from df_f\n",
    "group by h3_level8_index, create_yrweek\n",
    "order by h3_level8_index, create_yrweek\n",
    "'''\n",
    "\n",
    "df_f = sqldf.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We might have to fill in missing weeks\n",
    "fill_df = pd.DataFrame({'create_yrweek': [str(x) for x in range(202001,202054)], 'req':0})\n",
    "sa_calendar['yrweek'] = sa_calendar['date'].dt.year.astype(str) + sa_calendar['date'].dt.isocalendar().week.astype(str).map(\"{:0>2}\".format)\n",
    "sa_tmp = sa_calendar[['yrweek','desc']].groupby('yrweek').agg({'desc':'nunique'})\n",
    "fill_df['pubday'] = df.merge(sa_tmp, how='left', left_on='create_yrweek', right_on='yrweek')['desc']\n",
    "fill_df['pubday'] = fill_df['pubday'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can map the fill_df values now onto our forcast set\n",
    "df_f2 = []\n",
    "g = df_f.groupby('h3_level8_index')\n",
    "for name, data in g:\n",
    "    fill_df['h3_level8_index'] = name\n",
    "    df_f2.append(pd.concat([data,fill_df[~fill_df['create_yrweek'].isin(data['create_yrweek'])]]))\n",
    "\n",
    "df_f2 = pd.concat(df_f2)\n",
    "\n",
    "query = '''\n",
    "select h3_level8_index,\n",
    "       create_yrweek,\n",
    "       pubday,\n",
    "       req\n",
    "from df_f2\n",
    "order by h3_level8_index, create_yrweek\n",
    "'''\n",
    "\n",
    "df_f = sqldf.run(query)\n",
    "del df_f2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting model and prep\n",
    "We'll use a simple gradient boosting machine for this\n",
    "* Benefits are insensitivity to data transformations\n",
    "* Also we can use multivariate approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model using a loop\n",
    "from xgboost import XGBRegressor\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from skforecast.ForecasterAutoregMultiOutput import ForecasterAutoregMultiOutput\n",
    "from skforecast.model_selection import grid_search_forecaster\n",
    "from skforecast.model_selection import backtesting_forecaster\n",
    "\n",
    "g = df_f.groupby('h3_level8_index')\n",
    "pred_df = []\n",
    "\n",
    "exog_variables = ['pubday']\n",
    "end_train = 38\n",
    "end_validation = 43\n",
    "for name, data in g:\n",
    "    data = data.reset_index(drop=True)\n",
    "    data_train = data.loc[: end_train, :]\n",
    "    data_val   = data.loc[end_train:end_validation, :]\n",
    "    data_test  = data.loc[end_validation:, :]\n",
    "    \n",
    "    forecaster = ForecasterAutoreg(\n",
    "                regressor = XGBRegressor(),\n",
    "                lags = 4\n",
    "                )\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 500],\n",
    "        'max_depth': [3, 5, 10],\n",
    "        'learning_rate': [0.01, 0.1]\n",
    "        }\n",
    "\n",
    "    # Lags used as predictors\n",
    "    lags_grid = [4, [1, 2, 3, 4]]\n",
    "\n",
    "    results_grid = grid_search_forecaster(\n",
    "                            forecaster         = forecaster,\n",
    "                            y                  = data.loc[:end_validation, 'req'],\n",
    "                            exog               = data.loc[:end_validation, exog_variables],\n",
    "                            param_grid         = param_grid,\n",
    "                            #lags_grid          = lags_grid,\n",
    "                            steps              = 36,\n",
    "                            refit              = False,\n",
    "                            metric             = 'mean_squared_error',\n",
    "                            initial_train_size = int(len(data_train)),\n",
    "                            fixed_train_size   = False,\n",
    "                            return_best        = True,\n",
    "                            verbose            = False\n",
    "                            )\n",
    "    # Backtesting\n",
    "    metric, predictions = backtesting_forecaster(\n",
    "        forecaster         = forecaster,\n",
    "        y                  = data['req'],\n",
    "        exog               = data[exog_variables],\n",
    "        initial_train_size = len(data.loc[:end_validation]),\n",
    "        fixed_train_size   = False,\n",
    "        steps              = 36,\n",
    "        refit              = False,\n",
    "        metric             = 'mean_squared_error',\n",
    "        verbose            = False\n",
    "        )\n",
    "    pred = predictions[-4:].reset_index(drop=True)\n",
    "    pred['hex'] = name\n",
    "    pred['backtest_error'] = metric\n",
    "    pred_df.append(pred)\n",
    "    del forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.concat(pred_df)\n",
    "pred_df.to_csv('4-predictive_analytic_tasks_forecast.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ce8fb71a79b41f8c541fde538ed2a6b64f67a34e45f24ea94ca1e23049d923f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
